{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b195bce7",
   "metadata": {},
   "source": [
    "# 몬테카를로 컨트롤 구현\n",
    "업그레이드된 그리디 월드에서 몬테카를로 컨트롤을 구현해보자.\n",
    "이를 위해 간략화된 정책 이터레이션(평가와 개선 단계를 끝까지 진행하지 않고 얕게 진행하는 것)을 사용할 것이다. 즉, 굳이 수렴할 때까지 평가 단계를 진행하지 않고 일단 가치 테이블에 저장해둔 값들이 조금이라도 바뀌면 그에 대해 개선 단계를 진행해보자.\n",
    "\n",
    "**수렴할 때까지 반복**\n",
    "* 한 에피소드의 경험을 쌓고\n",
    "* 경험된 데이터로 $ q(s,a) $ 테이블의 값을 업데이트하고(정책 평가)\n",
    "* 업데이트된 $ q(s,a) $ 테이블을 이용하여 $ \\epsilon -greedy $ 정책을 만들고(정책 개선)\n",
    "\n",
    "위 과정을 반복한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df7fae85",
   "metadata": {},
   "source": [
    "## 라이브러리 import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab3d4251",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np # q(s,a)를 numpy array 형태로 관리하기 위함"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e606e27",
   "metadata": {},
   "source": [
    "## GridWorld 클래스"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ed65b94e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridWorld():\n",
    "    def __init__(self):\n",
    "        self.x = 0\n",
    "        self.y = 0\n",
    "    \n",
    "    def step(self, a):\n",
    "        # 0번 액션: 왼쪽, 1번 액션: 위, 2번 액션: 오른쪽, 3번 액션: 아래쪽\n",
    "        if a == 0:\n",
    "            self.move_left()\n",
    "        elif a == 1:\n",
    "            self.move_up()\n",
    "        elif a == 2:\n",
    "            self.move_right()\n",
    "        elif a == 3:\n",
    "            self.move_down()\n",
    "        \n",
    "        reward = -1 # 보상은 -1로 고정\n",
    "        done = self.is_done()\n",
    "        return (self.x, self.y), reward, done\n",
    "    \n",
    "    def move_right(self):\n",
    "        if self.y == 1 and self.x in [0, 1, 2]:\n",
    "            pass\n",
    "        elif self.y == 3 and self.x in [2, 3, 4]:\n",
    "            pass\n",
    "        elif self.y == 6:\n",
    "            pass\n",
    "        else:\n",
    "            self.y += 1\n",
    "        \n",
    "    def move_left(self):\n",
    "        if self.y == 0:\n",
    "            pass\n",
    "        elif self.y == 3 and self.x in [0, 1, 2]:\n",
    "            pass\n",
    "        elif self.y == 5 and self.x in [2, 3, 4]:\n",
    "            pass\n",
    "        else:\n",
    "            self.y -= 1\n",
    "    \n",
    "    def move_up(self):\n",
    "        if self.x == 0:\n",
    "            pass\n",
    "        elif self.x == 3 and self.y == 2:\n",
    "            pass\n",
    "        else:\n",
    "            self.x -= 1\n",
    "            \n",
    "    def move_down(self):\n",
    "        if self.x == 4:\n",
    "            pass\n",
    "        elif self.x == 1 and self.y == 4:\n",
    "            pass\n",
    "        else:\n",
    "            self.x += 1\n",
    "    \n",
    "    def is_done(self):\n",
    "        if self.x == 4 and self.y == 6: # 목표 지점인 (4, 6)에 도달\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    \n",
    "    def reset(self):\n",
    "        self.x = 0\n",
    "        self.y = 0\n",
    "        return (self.x, self.y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2db6ce3",
   "metadata": {},
   "source": [
    "## QAgent 클래스"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d428057",
   "metadata": {},
   "source": [
    "에이전트에 해당하는 객체를 정의해보자.\n",
    "에이전트 객체는 내부에 $q(s,a)$의 값을 저장하기 위한 테이블을 갖고 있고, 이 테이블은 실제로 에이전트가 액션을 선택할 때 사용된다. \n",
    "\n",
    "* `select_action` 함수를 통해 상태 $s$를 인풋으로 받아 $s$에서 알맞은 액션을 입실론 그리디 방식을 통해 선택한다.\n",
    "* `update_table` 함수는 실제로 테이블의 값을 업데이트 해주는 함수로, 하나의 에피소드에 해당하는 데이터를 받아서 MC 방법으로 테이블의 값을 업데이트 한다.\n",
    "* `anneal_eps` 함수는 epsilon의 값을 조금씩 줄여주는 함수이다.\n",
    "* `show_table` 함수는 학습이 끝난 후에 상태별로 $q(s,a)$의 값이 가장 큰 액션을 뽑아서 보여주는 함수이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "646ee64e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QAgent():\n",
    "    def __init__(self):\n",
    "        self.q_table = np.zeros((5, 7, 4)) # q밸류를 저장하는 변수. 모두 0으로 초기화\n",
    "        self.eps = 0.9\n",
    "        self.alpha = 0.01\n",
    "    \n",
    "    def select_action(self, s):\n",
    "        # eps-greedy로 액션을 선택\n",
    "        x, y = s\n",
    "        coin = random.random()\n",
    "        if coin < self.eps:\n",
    "            action = random.randint(0, 3)\n",
    "        else:\n",
    "            action_val = self.q_table[x, y,:]\n",
    "            action = np.argmax(action_val)\n",
    "        return action\n",
    "    \n",
    "    def update_table(self, history):\n",
    "        # 한 에피소드에 해당하는 history를 입력으로 받아 q 테이블의 값을 업데이트\n",
    "        cum_reward = 0\n",
    "        for transition in history[::-1]:\n",
    "            s, a, r, s_prime = transition\n",
    "            x, y = s\n",
    "            # 몬테카를로 방식을 사용하여 업데이트\n",
    "            self.q_table[x, y, a] = self.q_table[x, y, a] + self.alpha * (cum_reward - self.q_table[x, y, a])\n",
    "            cum_reward = cum_reward + r\n",
    "            \n",
    "    def anneal_eps(self):\n",
    "        self.eps -= 0.03\n",
    "        self.eps = max(self.eps, 0.1)\n",
    "    \n",
    "    def show_table(self):\n",
    "        # 학습이 각 위치에서 어느 액션의 q 값이 가장 높았는지 보여줌\n",
    "        q_list = self.q_table.tolist()\n",
    "        data = np.zeros((5, 7))\n",
    "        for row_idx in range(len(q_list)):\n",
    "            row = q_list[row_idx]\n",
    "            for col_idx in range(len(row)):\n",
    "                col = row[col_idx]\n",
    "                action = np.argmax(col)\n",
    "                data[row_idx, col_idx] = action\n",
    "        print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac453b8d",
   "metadata": {},
   "source": [
    "## Main 함수"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d54fa1",
   "metadata": {},
   "source": [
    "에이전트와 환경을 만들고, 하나의 에피소드가 끝날 때까지 `history`라는 변수에 상태 전이 과정을 모두 저장해두었다가, 에피소드가 끝나는 순간 해당 변수를 이용해서 에이전트 내부의 q 테이블을 업데이트한다. 그리고 epsilon의 값을 조금씩 줄여준다.\n",
    "1천 번의 에피소드 동안 학습을 하고 나면 최종 결과를 출력해준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "55f46393",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    env = GridWorld()\n",
    "    agent = QAgent()\n",
    "    \n",
    "    for n_epi in range(1000): # 총 1000 에피소드 동안 학습\n",
    "        done = False\n",
    "        history = []\n",
    "        \n",
    "        s = env.reset()\n",
    "        while not done: # 한 Q에피소드가 끝날 때까지\n",
    "            a = agent.select_action(s)\n",
    "            s_prime, r, done = env.step(a)\n",
    "            history.append((s, a, r, s_prime))\n",
    "            s = s_prime\n",
    "        agent.update_table(history)\n",
    "        agent.anneal_eps()\n",
    "        \n",
    "    agent.show_table() # 학습이 끝난 결과를 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b5c94d96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3. 0. 0. 2. 2. 3. 3.]\n",
      " [2. 3. 0. 2. 2. 3. 3.]\n",
      " [3. 3. 0. 1. 0. 3. 0.]\n",
      " [0. 3. 1. 1. 0. 3. 3.]\n",
      " [2. 2. 2. 1. 0. 2. 0.]]\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5646807",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
