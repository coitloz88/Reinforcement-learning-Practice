{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5abd0c9a",
   "metadata": {},
   "source": [
    "# Q-learning\n",
    "\n",
    "## Off-Policy 학습\n",
    "* On-Policy: 타깃 정책과 행동 정책이 같은 경우(직접 경험)\n",
    "* Off-Policy: 타깃 정책과 행동 정책이 다른 경우(간접 경험)\n",
    "    - 과거의 경험을 재사용할 수 있다.\n",
    "    - 사람의 데이터로부터 학습할 수 있다.\n",
    "    - 일대다, 다대일 학습이 가능하다.\n",
    "    \n",
    "## Q러닝의 이론적 배경: 벨만 최적 방정식\n",
    "최적 액션 밸류 $q_{*}(s,a)$는 이 세상에 존재하는 모든 정책들 중에 얻게 되는 가장 좋은 정책을 따를 때의 가치를 나타내는 함수이다.\n",
    "<center>\n",
    "    $q_{*}(s,a) = \\underset{\\pi}{max} q_{\\pi}(s,a)$\n",
    "</center>\n",
    "$q_{*}$를 알게 되면 주어진 MDP에서 순간마다 최적의 행동을 취하면서 움직일 수 있다. 상태마다 $q_{*}$의 값이 가장 높은 액션을 취하면 되기 때문이다.\n",
    "<center>\n",
    "    $\\pi_{*} = \\underset{a}{argmax} q_{\\pi}(s,a)$\n",
    "</center>\n",
    "\n",
    "그러나 최종적인 목표는 최적의 액션-가치 함수인 $q_{*}$를 찾는 것인데, 이는 벨만 최적 방정식을 기반으로 업데이트할 수 있다.\n",
    "<center>\n",
    "    벨만 최적 방정식: $q_{*}(s,a) = \\mathbb{E}_{s^{\\prime}}[r+\\gamma \\underset{a^{\\prime}}{max}q_{*}(s^{\\prime},a^{\\prime})]$  \n",
    "</center>\n",
    "<center>\n",
    "    Q러닝: $Q(S,A) \\rightarrow Q(S,A) + \\alpha (R + \\gamma \\underset{A^{\\prime}}{max}Q(S^{\\prime},A^{\\prime})-Q(S,A))$\n",
    "</center>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d2874f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
