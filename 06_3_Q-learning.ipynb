{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5abd0c9a",
   "metadata": {},
   "source": [
    "# Q-learning\n",
    "\n",
    "## Off-Policy 학습\n",
    "* On-Policy: 타깃 정책과 행동 정책이 같은 경우(직접 경험)\n",
    "* Off-Policy: 타깃 정책과 행동 정책이 다른 경우(간접 경험)\n",
    "    - 과거의 경험을 재사용할 수 있다.\n",
    "    - 사람의 데이터로부터 학습할 수 있다.\n",
    "    - 일대다, 다대일 학습이 가능하다.\n",
    "    \n",
    "## Q러닝의 이론적 배경: 벨만 최적 방정식\n",
    "최적 액션 밸류 $q_{*}(s,a)$는 이 세상에 존재하는 모든 정책들 중에 얻게 되는 가장 좋은 정책을 따를 때의 가치를 나타내는 함수이다.\n",
    "<center>\n",
    "    $q_{*}(s,a) = \\underset{\\pi}{max} q_{\\pi}(s,a)$\n",
    "</center>\n",
    "$q_{*}$를 알게 되면 주어진 MDP에서 순간마다 최적의 행동을 취하면서 움직일 수 있다. 상태마다 $q_{*}$의 값이 가장 높은 액션을 취하면 되기 때문이다.\n",
    "<center>\n",
    "    $\\pi_{*} = \\underset{a}{argmax} q_{\\pi}(s,a)$\n",
    "</center>\n",
    "\n",
    "그러나 최종적인 목표는 최적의 액션-가치 함수인 $q_{*}$를 찾는 것인데, 이는 벨만 최적 방정식을 기반으로 업데이트할 수 있다.\n",
    "<center>\n",
    "    벨만 최적 방정식: $q_{*}(s,a) = \\mathbb{E}_{s^{\\prime}}[r+\\gamma \\underset{a^{\\prime}}{max}q_{*}(s^{\\prime},a^{\\prime})]$  \n",
    "</center>\n",
    "<center>\n",
    "    Q러닝: $Q(S,A) \\rightarrow Q(S,A) + \\alpha (R + \\gamma \\underset{A^{\\prime}}{max}Q(S^{\\prime},A^{\\prime})-Q(S,A))$\n",
    "</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eeb8a60",
   "metadata": {},
   "source": [
    "# Q-learning 구현"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3419a78f",
   "metadata": {},
   "source": [
    "## 라이브러리 import 및 GridWorld 클래스"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1b969d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b608dcdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridWorld():\n",
    "    def __init__(self):\n",
    "        self.x = 0\n",
    "        self.y = 0\n",
    "    \n",
    "    def step(self, a):\n",
    "        # 0번 액션: 왼쪽, 1번 액션: 위, 2번 액션: 오른쪽, 3번 액션: 아래쪽\n",
    "        if a == 0:\n",
    "            self.move_left()\n",
    "        elif a == 1:\n",
    "            self.move_up()\n",
    "        elif a == 2:\n",
    "            self.move_right()\n",
    "        elif a == 3:\n",
    "            self.move_down()\n",
    "        \n",
    "        reward = -1 # 보상은 -1로 고정\n",
    "        done = self.is_done()\n",
    "        return (self.x, self.y), reward, done\n",
    "    \n",
    "    def move_right(self):\n",
    "        if self.y == 1 and self.x in [0, 1, 2]:\n",
    "            pass\n",
    "        elif self.y == 3 and self.x in [2, 3, 4]:\n",
    "            pass\n",
    "        elif self.y == 6:\n",
    "            pass\n",
    "        else:\n",
    "            self.y += 1\n",
    "        \n",
    "    def move_left(self):\n",
    "        if self.y == 0:\n",
    "            pass\n",
    "        elif self.y == 3 and self.x in [0, 1, 2]:\n",
    "            pass\n",
    "        elif self.y == 5 and self.x in [2, 3, 4]:\n",
    "            pass\n",
    "        else:\n",
    "            self.y -= 1\n",
    "    \n",
    "    def move_up(self):\n",
    "        if self.x == 0:\n",
    "            pass\n",
    "        elif self.x == 3 and self.y == 2:\n",
    "            pass\n",
    "        else:\n",
    "            self.x -= 1\n",
    "            \n",
    "    def move_down(self):\n",
    "        if self.x == 4:\n",
    "            pass\n",
    "        elif self.x == 1 and self.y == 4:\n",
    "            pass\n",
    "        else:\n",
    "            self.x += 1\n",
    "    \n",
    "    def is_done(self):\n",
    "        if self.x == 4 and self.y == 6: # 목표 지점인 (4, 6)에 도달\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    \n",
    "    def reset(self):\n",
    "        self.x = 0\n",
    "        self.y = 0\n",
    "        return (self.x, self.y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "da346dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QAgent():\n",
    "    def __init__(self):\n",
    "        self.q_table = np.zeros((5, 7, 4)) # q밸류를 저장하는 변수. 모두 0으로 초기화\n",
    "        self.eps = 0.9\n",
    "    \n",
    "    def select_action(self, s):\n",
    "        # eps-greedy로 액션을 선택\n",
    "        x, y = s\n",
    "        coin = random.random()\n",
    "        if coin < self.eps:\n",
    "            action = random.randint(0, 3)\n",
    "        else:\n",
    "            action_val = self.q_table[x, y,:]\n",
    "            action = np.argmax(action_val)\n",
    "        return action\n",
    "    \n",
    "    def update_table(self, transition):\n",
    "        s, a, r, s_prime = transition\n",
    "        x, y = s\n",
    "        next_x, next_y = s_prime\n",
    "        # Q-learning 업데이트 식을 이용\n",
    "        self.q_table[x, y, a] = self.q_table[x, y, a] + 0.1 * (r + np.amax(self.q_table[next_x, next_y,:] - self.q_table[x, y, a]))\n",
    "            \n",
    "    def anneal_eps(self):\n",
    "        self.eps -= 0.01 # Q-learning에서는 epsilon이 좀 더 천천히 줄어들도록 함\n",
    "        self.eps = max(self.eps, 0.2)\n",
    "    \n",
    "    def show_table(self):\n",
    "        # 학습이 각 위치에서 어느 액션의 q 값이 가장 높았는지 보여줌\n",
    "        q_list = self.q_table.tolist()\n",
    "        data = np.zeros((5, 7))\n",
    "        for row_idx in range(len(q_list)):\n",
    "            row = q_list[row_idx]\n",
    "            for col_idx in range(len(row)):\n",
    "                col = row[col_idx]\n",
    "                action = np.argmax(col)\n",
    "                data[row_idx, col_idx] = action\n",
    "        print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8399b13d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    env = GridWorld()\n",
    "    agent = QAgent()\n",
    "    \n",
    "    for n_epi in range(1000): # 총 1000 에피소드 동안 학습\n",
    "        done = False\n",
    "        \n",
    "        s = env.reset()\n",
    "        while not done:\n",
    "            a = agent.select_action(s)\n",
    "            s_prime, r, done = env.step(a)\n",
    "            agent.update_table((s, a, r, s_prime))\n",
    "            s = s_prime\n",
    "        agent.anneal_eps()\n",
    "        \n",
    "    agent.show_table() # 학습이 끝난 결과를 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1ab60e9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3. 3. 0. 2. 2. 3. 3.]\n",
      " [3. 3. 0. 2. 2. 3. 3.]\n",
      " [3. 3. 0. 1. 0. 3. 3.]\n",
      " [2. 2. 2. 1. 0. 3. 3.]\n",
      " [0. 2. 2. 1. 0. 2. 0.]]\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d9e2123",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
