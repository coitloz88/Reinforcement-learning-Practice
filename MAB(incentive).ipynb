{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba450abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from audioop import mul\n",
    "import numpy as np\n",
    "from abc import abstractmethod\n",
    "from collections import defaultdict\n",
    "from typing import Dict, Optional\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import accuracy_score, log_loss\n",
    "from soupsieve import select\n",
    "\"\"\"\n",
    "Two Objectives:\n",
    "    1. Maximize E(success)\n",
    "    2. Minimize E(cost)\n",
    "incentive 금액을 다양한 방법에 기반하여 조절하면서 Objective를 가장 최적으로 달성하는 모델을 찾는다.\n",
    "\"\"\"\n",
    "\n",
    "class BaseIncentive:\n",
    "    def __init__(\n",
    "        self, incentives: np.ndarray, \n",
    "        random_state: int = None,\n",
    "        **kwargs\n",
    "    ) -> None:\n",
    "        self._incentives = np.asarray(incentives)\n",
    "        self._random = np.random.default_rng(random_state)\n",
    "\n",
    "    @abstractmethod\n",
    "    def expect(self, incentive: float, context: any=None) -> float:\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def choose(self, context: any=None) -> float:\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def update(self, incentive: float, response: float, context: any=None) -> Optional[Dict[str, any]]:\n",
    "        pass\n",
    "\n",
    "\n",
    "class LogisticRegressionIncentive(BaseIncentive): # Update 시 Logistic Regression 모델 사용\n",
    "    def __init__(\n",
    "        self, \n",
    "        incentives: np.ndarray, \n",
    "        incentive_default: float, \n",
    "        expected_likelihood: float, \n",
    "        window_size: int,\n",
    "        allow_exploration: bool = False,\n",
    "        random_state: int = None,\n",
    "        **kwargs\n",
    "    ) :\n",
    "        super().__init__(incentives, random_state, **kwargs)\n",
    "\n",
    "        self._incentive_default = incentive_default\n",
    "        self._expected_likelihood = expected_likelihood\n",
    "        self._window_size = window_size\n",
    "        self._allow_exploration = allow_exploration\n",
    "\n",
    "        self._lr = None\n",
    "        self._encoder = None\n",
    "        self._histories = []\n",
    "    \n",
    "    \n",
    "    @property\n",
    "    def histories_(self): # 이전 기록\n",
    "        return self._histories[-self._window_size:] if self._window_size > 0 else self._histories\n",
    "    \n",
    "    @property\n",
    "    def coef_incentive_(self): # 회귀 계수\n",
    "        if self._lr:\n",
    "            return np.ravel(self._lr.coef_)[0]\n",
    "        else:\n",
    "            return np.nan\n",
    "\n",
    "    def expect(self, incentive: float, context: any = None) -> float:\n",
    "        assert self._lr is not None\n",
    "        assert self._encoder is not None        \n",
    "        \n",
    "        categories = self._encoder.categories_[0]\n",
    "        if context not in categories:\n",
    "            context = np.zeros((1, len(categories)))\n",
    "        else:\n",
    "            context = self._encoder.transform(np.asarray([[context]], dtype=object))\n",
    "        \n",
    "        incentive = np.asarray([[incentive]])\n",
    "        X = np.concatenate((incentive, context), axis=1)\n",
    "        return np.ravel(self._lr.predict_proba(X=X)[:, 1]).item(0)\n",
    "\n",
    "    def choose(self, context: any = None) -> float:\n",
    "        if self._lr is None or self._encoder is None:\n",
    "            responses = np.asarray([i for _, _, i in self.histories_]) # history(성공/실패 기록)의 reponse 데이터를 배열로 변형\n",
    "            \n",
    "            if len(responses) == 0: # reponse가 없는 경우 default incentive를 return\n",
    "                return self._incentive_default\n",
    "            elif np.all(responses == 1.0): # 모든 reponse가 성공인 경우 기록 중 minimum incentive를 return\n",
    "                return np.min(self._incentives)\n",
    "            elif np.all(responses == 0.0): # 모든 reponse가 실패인 경우 기록 중 maximum incentive를 return\n",
    "                return np.max(self._incentives)\n",
    "\n",
    "        if self.coef_incentive_ < 1e-3: # 기울기 소실?\n",
    "            if self._allow_exploration:\n",
    "                return self._random.choice(self._incentives)\n",
    "            else:\n",
    "                return np.min(self._incentives)\n",
    "        probs = np.asarray([\n",
    "            self.expect(incentive=incentive, context=context) for incentive in self._incentives\n",
    "        ])\n",
    "        diff = np.abs(probs - self._expected_likelihood)\n",
    "\n",
    "        opt_incentives = self._incentives[np.where(diff == np.min(diff))] # select or incentives that have minimum diff\n",
    "\n",
    "        return self._random.choice(opt_incentives) # among optimal incentives, return random incentive\n",
    "\n",
    "\n",
    "    def update(self, incentive: float, response: float, context: any = None) -> Optional[Dict[str, any]]:\n",
    "        self._histories.append((context, incentive, response)) # context, incentive, reponsce 기록 추가\n",
    "\n",
    "        histories = self.histories_\n",
    "        responses = np.asarray([i for _, _, i in histories])\n",
    "        \n",
    "        if len(np.unique(responses)) == 1:\n",
    "            return None\n",
    "\n",
    "        contexts = np.asarray([[i] for i, _, _ in histories])\n",
    "        incentives = np.asarray([[i] for _, i, _ in histories])\n",
    "\n",
    "        self._encoder = OneHotEncoder(sparse=False, dtype=np.int32, drop=None).fit(contexts) # one-hot 인코딩\n",
    "\n",
    "        X = np.concatenate((incentives, self._encoder.transform(contexts)), axis=1)\n",
    "        self._lr = LogisticRegression(C=0.1, tol=1e-4, solver='lbfgs', penalty='l2').fit(X, responses) # Logistic Regression 모델 생성\n",
    "        \n",
    "        y_prob = self._lr.predict_proba(X)[:, 1]  # 각 샘플에 대해 클래스 X에 속할 확률 리턴\n",
    "        y_pred = self._lr.predict(X) # 속성이 X에 속하는지 아닌지 알려주는 벡터 리턴\n",
    "\n",
    "        return {'accuracy': accuracy_score(responses, y_pred), 'logloss': log_loss(responses, y_prob)} # 평가 결과 리턴\n",
    "\n",
    "\n",
    "class FixedIncentive(BaseIncentive): # 고정된 incentive 리턴\n",
    "    def __init__(\n",
    "        self, \n",
    "        incentives: np.ndarray, \n",
    "        incentive_default: float,\n",
    "        random_state: int = None,\n",
    "        **kwargs\n",
    "    ) -> None:\n",
    "        super().__init__(incentives, random_state, **kwargs)\n",
    "        self._incentive_default = incentive_default\n",
    "\n",
    "    def expect(self, incentive: float, context: any = None) -> float:\n",
    "        return np.nan\n",
    "\n",
    "    def choose(self, context: any = None) -> float:\n",
    "        return self._incentive_default\n",
    "\n",
    "    def update(self, incentive: float, response: float, context: any = None) -> Optional[Dict[str, any]]:\n",
    "        return\n",
    "\n",
    "\n",
    "class RandomIncentive(BaseIncentive): # 랜덤한 incentive 리턴\n",
    "    def __init__( \n",
    "        self, \n",
    "        incentives: np.ndarray, \n",
    "        random_state: int = None,\n",
    "        **kwargs\n",
    "    ) -> None:\n",
    "        super().__init__(incentives, random_state, **kwargs)\n",
    "\n",
    "    def expect(self, incentive: float, context: any = None) -> float:\n",
    "        return 1.0 / len(self._incentives)\n",
    "\n",
    "    def choose(self, context: any = None) -> float:\n",
    "        return self._random.choice(self._incentives)\n",
    "\n",
    "    def update(self, incentive: float, response: float, context: any = None) -> Optional[Dict[str, any]]:\n",
    "        return\n",
    "\n",
    "\n",
    "class ThompsonSamplingIncentive(BaseIncentive):\n",
    "    def __init__(\n",
    "        self, \n",
    "        incentives: np.ndarray, \n",
    "        random_state: int=None,\n",
    "        is_optimistic: bool=False,\n",
    "        multi_objective: str=None,\n",
    "        **kwargs\n",
    "    ) -> None:\n",
    "        super().__init__(incentives, random_state, **kwargs)\n",
    "\n",
    "        if multi_objective is not None and multi_objective not in ('random', 'success', 'cost'):\n",
    "            raise ValueError('the argument, \"select\", should be one of \"random\", \"success\", or \"cost\".')\n",
    "\n",
    "        self._is_optimistic = is_optimistic\n",
    "        self._multi_objective = multi_objective\n",
    "        \n",
    "        self._init_arms()\n",
    "        \n",
    "    def expect(self, incentive: float, context: any = None) -> float:\n",
    "        i = np.flatnonzero(self._incentives == incentive).item(0)\n",
    "        a, b = self._arms[context][i]\n",
    "        theta = self._random.beta(a + 1, b + 1)\n",
    "        if self._is_optimistic:\n",
    "            theta = np.max((theta, a / (a+b)))\n",
    "        return theta\n",
    "\n",
    "    def choose(self, context: any = None) -> float: # multi objective를 만족하는 context(의 incentive)를 고르거나 혹은 expect가 최대가 되게 하는 하나만 고른다\n",
    "        return self._choose_multi(context=context) if self._multi_objective else self._choose_single(context)\n",
    "\n",
    "    def _init_arms(self):\n",
    "        self._arms = defaultdict(lambda: np.zeros((len(self._incentives), 2)))\n",
    "\n",
    "    def _choose_single(self, context: any = None) -> float:\n",
    "        expects = np.asarray([\n",
    "            self.expect(incentive=incentive, context=context) for incentive in self._incentives\n",
    "        ])\n",
    "        opts = self._incentives[expects == np.max(expects)]\n",
    "        return self._random.choice(opts)\n",
    "\n",
    "    def _choose_multi(self, context: any = None) -> float:\n",
    "        obj_success_ratio = np.asarray([\n",
    "            self.expect(incentive=incentive, context=context) for incentive in self._incentives\n",
    "        ])\n",
    "        obj_cost = obj_success_ratio * self._incentives\n",
    "        orders = np.asarray([1, -1])\n",
    "        objectives = np.column_stack((obj_success_ratio, obj_cost))\n",
    "        I_opt = self._find_fareto_frontiers(objectives, orders)\n",
    "        if len(I_opt):\n",
    "            if self._multi_objective == 'random':\n",
    "                i_opt = self._random.choice(I_opt)\n",
    "            elif self._multi_objective == 'success':\n",
    "                objectives_opt = obj_success_ratio[I_opt]\n",
    "                i_opt = np.argmax(objectives_opt)\n",
    "            elif self._multi_objective == 'cost':\n",
    "                objectives_opt = obj_cost[I_opt]\n",
    "                i_opt = np.argmin(objectives_opt)\n",
    "            return self._incentives[i_opt]\n",
    "        else:\n",
    "            return self._random.choice(self._incentives)\n",
    "    \n",
    "    def _is_dominated(self, x: int, y: int, objectives: np.ndarray, orders: np.ndarray) -> bool: # 우선순위 결정\n",
    "        \"\"\"\n",
    "        At least one objective is better than another, \n",
    "        and other objectives are equal to or better than others.\n",
    "        \"\"\"\n",
    "        n_objectives = objectives.shape[1]\n",
    "\n",
    "        for i in np.arange(n_objectives):\n",
    "            order = orders[i]\n",
    "            if order > 0:\n",
    "                is_dominate = objectives[x, i] > objectives[y, i]\n",
    "            else:\n",
    "                is_dominate = objectives[x, i] < objectives[y, i]\n",
    "\n",
    "            for j in np.arange(n_objectives):\n",
    "                if i == j:\n",
    "                    continue\n",
    "                else:\n",
    "                    if order > 0:\n",
    "                        is_dominate = is_dominate and objectives[x, j] >= objectives[y, j]\n",
    "                    else:\n",
    "                        is_dominate = is_dominate and objectives[x, j] <= objectives[y, j]\n",
    "\n",
    "            if is_dominate:\n",
    "                return True\n",
    "\n",
    "        return False\n",
    "\n",
    "    def _is_incomparable(self, x: int, y: int, objectives: np.ndarray, orders: np.ndarray) -> bool: # 비교 불가 여부 판단\n",
    "        \"\"\"\n",
    "        At least one objective is better than another, \n",
    "        but at least another one objective is worse than others.\n",
    "        \"\"\"\n",
    "        n_objectives = objectives.shape[1]\n",
    "\n",
    "        for i in np.arange(n_objectives):\n",
    "            order = orders[i]\n",
    "            if order > 0:\n",
    "                is_dominate = objectives[x, i] > objectives[y, i]\n",
    "            else:\n",
    "                is_dominate = objectives[x, i] < objectives[y, i]\n",
    "\n",
    "            for j in np.arange(n_objectives):\n",
    "                if order > 0:\n",
    "                    if i != j and objectives[x, j] < objectives[y, j] and is_dominate:\n",
    "                        return True\n",
    "                else:\n",
    "                     if i != j and objectives[x, j] > objectives[y, j] and is_dominate:\n",
    "                        return True\n",
    "        return False\n",
    "    \n",
    "    def _find_fareto_frontiers(self, objectives: np.ndarray, orders: np.ndarray) -> np.ndarray: # 동일한 우선순위에 있는 frontier들을 모두 리턴\n",
    "        \"\"\"\n",
    "        One objective is dominated or incomparable toward all other objectives,\n",
    "        such objective is the pareto frontier.\n",
    "        \"\"\"\n",
    "        frontiers = []\n",
    "        n = len(objectives)\n",
    "\n",
    "        for i in np.arange(n):\n",
    "            is_pareto_frontier = True\n",
    "            \n",
    "            for j in np.arange(n):\n",
    "                if i == j:\n",
    "                    continue\n",
    "                else:\n",
    "                    is_dominated = self._is_dominated(i, j, objectives, orders)\n",
    "                    is_incomparable = self._is_incomparable(i, j, objectives, orders)\n",
    "                    is_pareto_frontier = is_pareto_frontier and (is_dominated or is_incomparable)\n",
    "                    \n",
    "            if is_pareto_frontier:\n",
    "                frontiers.append(i)\n",
    "                \n",
    "        return np.asarray(frontiers)\n",
    "\n",
    "\n",
    "class SlidingWindowTSIncentive(ThompsonSamplingIncentive):\n",
    "    def __init__(\n",
    "        self, \n",
    "        incentives: np.ndarray, \n",
    "        window_size: int,\n",
    "        random_state: int = None, \n",
    "        is_optimistic: bool = False, \n",
    "        multi_objective: str = None, \n",
    "        **kwargs\n",
    "    ) -> None:\n",
    "        super().__init__(incentives, random_state, is_optimistic, multi_objective, **kwargs)\n",
    "        self._window_size = window_size\n",
    "        self._histories = []\n",
    "\n",
    "    @property\n",
    "    def histories_(self):\n",
    "        return self._histories[-self._window_size:] if self._window_size > 0 else self._histories\n",
    "\n",
    "    def update(self, incentive: float, response: float, context: any = None) -> Optional[Dict[str, any]]:\n",
    "        self._histories.append((context, incentive, response))\n",
    "        self._init_arms()\n",
    "\n",
    "        histories = self.histories_\n",
    "        contexts = np.asarray([i for i, _, _ in histories])\n",
    "        incentives = np.asarray([i for _, i, _ in histories])\n",
    "        responses = np.asarray([i for _, _, i in histories])\n",
    "        \n",
    "        for ctx in np.unique(contexts):\n",
    "            M_ctx = contexts == ctx\n",
    "            inc_ctx = incentives[M_ctx]\n",
    "            res_ctx = responses[M_ctx]\n",
    "            \n",
    "            for idx, inc in enumerate(self._incentives):\n",
    "                res = res_ctx[inc_ctx == inc]\n",
    "                a, b = np.sum(res == 1.0), np.sum(res == 0.0)\n",
    "                self._arms[ctx][idx] = (a, b)\n",
    "\n",
    "\n",
    "class DecayTSIncentive(ThompsonSamplingIncentive):\n",
    "    def __init__(\n",
    "        self, \n",
    "        incentives: np.ndarray, \n",
    "        decay_factor: float,\n",
    "        random_state: int = None, \n",
    "        is_optimistic: bool = False, \n",
    "        multi_objective: str = None, \n",
    "        **kwargs\n",
    "    ) -> None:\n",
    "        super().__init__(incentives, random_state, is_optimistic, multi_objective, **kwargs)\n",
    "        self._decay_factor = decay_factor\n",
    "\n",
    "    def update(self, incentive: float, response: float, context: any = None) -> Optional[Dict[str, any]]:\n",
    "        for ctx in self._arms.keys():\n",
    "            self._arms[ctx] = self._arms[ctx] * self._decay_factor\n",
    "        \n",
    "        i = np.flatnonzero(self._incentives == incentive).item(0)\n",
    "        a, b = self._arms[context][i]\n",
    "        a = a + response\n",
    "        b = b + (1.0 - response)\n",
    "        self._arms[context][i] = (a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "563ce785",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
